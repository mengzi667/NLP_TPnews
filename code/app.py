import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F
import pandas as pd
import plotly.express as px
import os

# --- 1. é¡µé¢é…ç½® (Page Configuration) ---
# st.set_page_config() å¿…é¡»æ˜¯è„šæœ¬ä¸­ç¬¬ä¸€ä¸ªè¢«è°ƒç”¨çš„Streamlitå‘½ä»¤
st.set_page_config(
    page_title="åŸå¸‚äº¤é€šèˆ†æƒ…æ„å›¾è¯†åˆ«ç³»ç»Ÿ",
    page_icon="ğŸš‡",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- 2. åŠ è½½æ¨¡å‹ (Model Loading) ---
# ä½¿ç”¨ @st.cache_resource è£…é¥°å™¨æ¥ç¼“å­˜æ¨¡å‹ï¼Œé¿å…æ¯æ¬¡åˆ·æ–°é¡µé¢éƒ½é‡æ–°åŠ è½½
@st.cache_resource
def load_model():
    """
    ä»Hugging Face HubåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚
    è¿™ä¸ªå‡½æ•°åªä¼šåœ¨ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶æ‰§è¡Œï¼Œä¹‹åçš„ç»“æœä¼šè¢«ç¼“å­˜ã€‚
    """
    # TODO: (éå¸¸é‡è¦!) å°†è¿™é‡Œçš„IDæ›¿æ¢ä¸ºä½ è‡ªå·±åœ¨Hugging Face Hubä¸Šçš„æ¨¡å‹ID
    # æ ¼å¼é€šå¸¸æ˜¯ "ä½ çš„ç”¨æˆ·å/ä½ çš„æ¨¡å‹å"
    model_id = "Mengzi667/macbert-traffic-intent-classifier"

    try:
        st.info(f"æ­£åœ¨ä»Hugging Face HubåŠ è½½æ¨¡å‹: {model_id} ...")
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        model = AutoModelForSequenceClassification.from_pretrained(model_id)
        st.success("æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸï¼")
        return tokenizer, model
    except Exception as e:
        st.error(f"ä»Hugging Face HubåŠ è½½æ¨¡å‹å¤±è´¥ã€‚è¯·ç¡®è®¤ï¼š")
        st.error(f"1. æ¨¡å‹ID '{model_id}' æ˜¯å¦æ­£ç¡®ã€‚")
        st.error(f"2. ä½ çš„æ¨¡å‹ä»“åº“æ˜¯å¦å·²è®¾ä¸ºå…¬å¼€ã€‚")
        st.error(f"3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸ã€‚")
        st.error(f"é”™è¯¯è¯¦æƒ…: {e}")
        return None, None

# æ‰§è¡ŒåŠ è½½
tokenizer, model = load_model()

# --- 3. é¡µé¢æ ‡é¢˜å’Œç®€ä»‹ (Title and Introduction) ---
st.title("ğŸš‡ åŸå¸‚äº¤é€šèˆ†æƒ…æ„å›¾è¯†åˆ«ç³»ç»Ÿ")
st.markdown("""
æ¬¢è¿ä½¿ç”¨æœ¬ç³»ç»Ÿï¼è¿™æ˜¯ä¸€ä¸ªåŸºäº `MacBERT` å¾®è°ƒæ¨¡å‹çš„è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«å…³äº**åŸå¸‚äº¤é€š**ï¼ˆåœ°é“ã€å…¬äº¤ã€å‡ºç§Ÿè½¦ç­‰ï¼‰çš„æ–‡æœ¬æ‰€è¡¨è¾¾çš„æ ¸å¿ƒæ„å›¾ã€‚
æ‚¨å¯ä»¥åœ¨ä¸‹æ–¹çš„æ–‡æœ¬æ¡†ä¸­è¾“å…¥ä»»ä½•åé¦ˆã€é—®é¢˜æˆ–å»ºè®®ï¼Œæ¨¡å‹å°†ä¼šç»™å‡ºå®ƒçš„åˆ¤æ–­ã€‚
""")
st.markdown("---")


# --- 4. ä¸»åº”ç”¨ç•Œé¢ (Main Interface) ---
if model is None:
    st.warning("æ¨¡å‹æœªèƒ½æˆåŠŸåŠ è½½ï¼Œåº”ç”¨æ— æ³•æ­£å¸¸å·¥ä½œã€‚è¯·æ£€æŸ¥ç»ˆç«¯æˆ–éƒ¨ç½²æ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯ã€‚")
else:
    col1, col2 = st.columns([2, 1]) # åˆ›å»ºä¸¤åˆ—ï¼Œå·¦è¾¹æ›´å®½

    with col1:
        st.header("âœï¸ è¯·è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬:")
        # ä½¿ç”¨è¡¨å•æ¥ç»„ç»‡è¾“å…¥å’ŒæŒ‰é’®ï¼Œå¯ä»¥é˜²æ­¢æ¯æ¬¡è¾“å…¥éƒ½é‡æ–°è¿è¡Œæ•´ä¸ªé¡µé¢
        with st.form("intent_form"):
            text_input = st.text_area(
                "è¾“å…¥æ–‡æœ¬:", 
                "å¼ºçƒˆå»ºè®®åœ°é“11å·çº¿åœ¨æ—©é«˜å³°å¢åŠ å‡ ç­è½¦ï¼Œç°åœ¨ç­‰ä¸€è¶Ÿçš„æ—¶é—´ä¹Ÿå¤ªé•¿äº†ï¼", 
                height=200,
                placeholder="ä¾‹å¦‚ï¼šä¸Šæµ·åœ°é“çš„ç©ºè°ƒæ˜¯æ‰“ç®—æŠŠäººå†»æ­»å—ï¼Ÿ"
            )
            submitted = st.form_submit_button("ğŸš€ å¼€å§‹åˆ†æ")

    with col2:
        st.header("ğŸ’¡ æ„å›¾ç±»åˆ«è¯´æ˜")
        st.info("""
        - **å’¨è¯¢:** è¯¢é—®ä¿¡æ¯ã€æµç¨‹ç­‰ã€‚
        - **æ±‚åŠ©:** è¯·æ±‚å…·ä½“å¸®åŠ©ï¼ˆå¦‚å¯»ç‰©ï¼‰ã€‚
        - **æŠ•è¯‰:** è¡¨è¾¾å¼ºçƒˆä¸æ»¡ã€‚
        - **ä¸¾æŠ¥:** æ­å‘è¿è§„æˆ–å®‰å…¨éšæ‚£ã€‚
        - **å»ºè®®:** æå‡ºå…·ä½“æ”¹è¿›æ–¹æ¡ˆã€‚
        - **æ­£é¢åé¦ˆ:** æ˜ç¡®çš„èµæ‰¬æˆ–æ„Ÿè°¢ã€‚
        """)

    # --- 5. æ¨¡å‹æ¨ç†ä¸ç»“æœå±•ç¤º (Inference and Display) ---
    if submitted:
        if not text_input.strip():
            st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹åå†è¿›è¡Œåˆ†æã€‚")
        else:
            with st.spinner('æ¨¡å‹æ­£åœ¨å…¨åŠ›åˆ†æä¸­ï¼Œè¯·ç¨å€™...'):
                # a. é¢„å¤„ç†è¾“å…¥
                inputs = tokenizer(text_input, return_tensors="pt", truncation=True, padding=True, max_length=256)
                
                # b. æ¨¡å‹æ¨ç†
                with torch.no_grad(): # åœ¨æ¨ç†æ¨¡å¼ä¸‹ï¼Œä¸è®¡ç®—æ¢¯åº¦ä»¥åŠ é€Ÿ
                    outputs = model(**inputs)
                
                # c. åå¤„ç†ç»“æœ
                logits = outputs.logits
                probabilities = F.softmax(logits, dim=-1).flatten().tolist()
                
                # è·å–æ ‡ç­¾åç§°å’Œå¯¹åº”çš„æ¦‚ç‡
                labels = list(model.config.id2label.values())
                prob_df = pd.DataFrame({
                    'æ„å›¾ç±»åˆ«': labels,
                    'æ¦‚ç‡': probabilities
                }).sort_values(by='æ¦‚ç‡', ascending=False).reset_index(drop=True)

                # d. å±•ç¤ºç»“æœ
                st.markdown("---")
                st.header("ğŸ“ˆ åˆ†æç»“æœ")
                
                res_col1, res_col2 = st.columns(2)
                
                with res_col1:
                    st.metric(
                        label="**æœ€é«˜æ¦‚ç‡æ„å›¾**", 
                        value=prob_df.iloc[0]['æ„å›¾ç±»åˆ«'],
                        help=f"æ¨¡å‹è®¤ä¸ºè¿™æ¡æ–‡æœ¬æœ€æœ‰å¯èƒ½å±äºè¿™ä¸ªç±»åˆ«ï¼Œç½®ä¿¡åº¦ä¸º {prob_df.iloc[0]['æ¦‚ç‡']:.2%}"
                    )
                    st.write("è¯¦ç»†æ¦‚ç‡åˆ†å¸ƒï¼š")
                    st.dataframe(prob_df,
                                  use_container_width=True,
                                  column_config={
                                      "æ¦‚ç‡": st.column_config.ProgressColumn(
                                          "æ¦‚ç‡", format="%.2f%%", min_value=0, max_value=1,
                                      ),
                                  },
                                  hide_index=True)

                with res_col2:
                    fig = px.bar(
                        prob_df, x='æ¦‚ç‡', y='æ„å›¾ç±»åˆ«', orientation='h', 
                        title='å„æ„å›¾ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒå›¾', text_auto='.2%',
                        color_discrete_sequence=px.colors.sequential.Blues_r
                    )
                    fig.update_layout(yaxis={'categoryorder':'total ascending'}, xaxis_title="æ¦‚ç‡", yaxis_title="æ„å›¾ç±»åˆ«")
                    st.plotly_chart(fig, use_container_width=True)

# --- 6. ä¾§è¾¹æ  (Sidebar) ---
st.sidebar.header("å…³äºæœ¬é¡¹ç›®")
st.sidebar.info(
    """
    è¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„NLPé¡¹ç›®ï¼Œæ—¨åœ¨é€šè¿‡AIæŠ€æœ¯èµ‹èƒ½åŸå¸‚äº¤é€šç®¡ç†ï¼Œè‡ªåŠ¨åˆ†ææµ·é‡ç¤¾æƒ…æ°‘æ„ï¼Œæå‡å“åº”æ•ˆç‡ã€‚
    
    **æ ¸å¿ƒæŠ€æœ¯æ ˆ:**
    - **æ™ºèƒ½æ ‡æ³¨:** LLM API (`é€šä¹‰åƒé—®`), Prompt Engineering
    - **æ¨¡å‹è®­ç»ƒ:** `PyTorch`, `Hugging Face Transformers`
    - **æ¨¡å‹é€‰å‹:** `MacBERT` vs `RoBERTa` (A/B Test)
    - **æ€§èƒ½ä¼˜åŒ–:** ç±»åˆ«åŠ æƒ, è¶…å‚æ•°æœç´¢ (`Optuna`)
    - **åº”ç”¨æ„å»º:** `Streamlit`, `Plotly`
    """
)
st.sidebar.markdown("---")
st.sidebar.write("**å¼€å‘è€…:** æ½˜ç¦¹èŒ ç§¦èµ«")
# TODO: æ›¿æ¢æˆä½ è‡ªå·±çš„GitHubé¡¹ç›®é“¾æ¥
st.sidebar.write("**GitHub:** [https://github.com/mengzi667/NLP_TPnews](https://github.com/mengzi667/NLP_TPnews)")
