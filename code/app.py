import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F
import pandas as pd
import plotly.express as px
import os

# --- 1. é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="åŸå¸‚äº¤é€šèˆ†æƒ…æ„å›¾è¯†åˆ«ç³»ç»Ÿ",
    page_icon="ğŸš‡",
    layout="wide",
    initial_sidebar_state="expanded",
    theme="light"
)

# --- 2. åŠ è½½æ¨¡å‹ ---
@st.cache_resource
def load_model():
    """ä»Hugging Face HubåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚"""
    model_id = "Mengzi667/macbert-traffic-intent-classifier" 
    try:
        st.info(f"æ­£åœ¨ä»äº‘ç«¯Hugging Face HubåŠ è½½æ¨¡å‹: {model_id} ...")
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        model = AutoModelForSequenceClassification.from_pretrained(model_id)
        st.success("æ¨¡å‹å’Œåˆ†è¯å™¨å·²æˆåŠŸä»äº‘ç«¯åŠ è½½ï¼")
        return tokenizer, model
    except Exception as e:
        st.error(f"ä»Hugging Face HubåŠ è½½æ¨¡å‹å¤±è´¥ã€‚è¯·ç¡®è®¤æ¨¡å‹IDã€ä»“åº“å…¬å¼€çŠ¶æ€åŠç½‘ç»œè¿æ¥ã€‚")
        st.error(f"é”™è¯¯è¯¦æƒ…: {e}")
        return None, None

tokenizer, model = load_model()

# --- 3. é¡µé¢æ ‡é¢˜å’Œç®€ä»‹ ---
st.title("ğŸš‡ åŸå¸‚äº¤é€šèˆ†æƒ…æ„å›¾è¯†åˆ«ç³»ç»Ÿ")
st.markdown("""
æ¬¢è¿ä½¿ç”¨æœ¬ç³»ç»Ÿï¼è¿™æ˜¯ä¸€ä¸ªåŸºäº `MacBERT` å¾®è°ƒæ¨¡å‹çš„è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«å…³äº**åŸå¸‚äº¤é€š**ï¼ˆåœ°é“ã€å…¬äº¤ã€å‡ºç§Ÿè½¦ç­‰ï¼‰çš„æ–‡æœ¬æ‰€è¡¨è¾¾çš„æ ¸å¿ƒæ„å›¾ã€‚
""")
st.markdown("---")

# --- 4. ä¸»åº”ç”¨ç•Œé¢ ---
if model is None:
    st.warning("æ¨¡å‹æœªèƒ½æˆåŠŸåŠ è½½ï¼Œåº”ç”¨æ— æ³•æ­£å¸¸å·¥ä½œã€‚")
else:
    # --- é¢„è®¾çš„è§£è¯»æ–‡æ¡ˆ ---
    INTERPRETATION_GUIDE = {
        "æŠ•è¯‰ (Complaint)": "æ¨¡å‹è¯†åˆ«å‡ºæ–‡æœ¬ä¸­å«æœ‰å¼ºçƒˆçš„è´Ÿé¢æƒ…ç»ªå’Œå¯¹ç°çŠ¶çš„ä¸æ»¡ï¼Œè¿™é€šå¸¸æŒ‡å‘ä¸€ä¸ªéœ€è¦è¢«è§£å†³çš„é—®é¢˜ã€‚",
        "å»ºè®® (Suggestion)": "æ¨¡å‹æ•æ‰åˆ°äº†ä¸€ä¸ªå…·ä½“çš„æ”¹è¿›æƒ³æ³•æˆ–æ–¹æ¡ˆã€‚è¿™ç±»åé¦ˆå¯¹ä¼˜åŒ–æœåŠ¡éå¸¸æœ‰ä»·å€¼ã€‚",
        "å’¨è¯¢ (Consultation)": "æ–‡æœ¬çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªä¿¡æ¯é—®è¯¢ï¼Œç”¨æˆ·å¸Œæœ›è·å¾—ä¸€ä¸ªå®¢è§‚çš„ç­”æ¡ˆã€‚",
        "æ±‚åŠ© (Request for Help)": "è¿™æ˜¯ä¸€ä¸ªæ˜ç¡®çš„æ±‚åŠ©ä¿¡å·ï¼Œé€šå¸¸ä¸ä¸ªäººé‡åˆ°çš„å…·ä½“å›°éš¾ï¼ˆå¦‚å¤±ç‰©ï¼‰ç›¸å…³ã€‚",
        "ä¸¾æŠ¥ (Report)": "æ¨¡å‹è¯†åˆ«å‡ºäº†å¯¹æŸä¸ªå…·ä½“è¿è§„è¡Œä¸ºæˆ–å®‰å…¨éšæ‚£çš„æ­å‘ï¼Œéœ€è¦ç›¸å…³éƒ¨é—¨å…³æ³¨ã€‚",
        "æ­£é¢åé¦ˆ (Positive Feedback)": "æ–‡æœ¬è¡¨è¾¾äº†æ˜ç¡®çš„èµæ‰¬æˆ–æ„Ÿè°¢ï¼Œæ˜¯æå‡æœåŠ¡ä¿¡å¿ƒçš„é‡è¦æ¥æºã€‚",
    }

    col1, col2 = st.columns([2, 1]) 

    with col1:
        st.header("âœï¸ è¯·è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬:")
        with st.form("intent_form"):
            text_input = st.text_area(
                "è¾“å…¥æ–‡æœ¬:", 
                "å¼ºçƒˆå»ºè®®åœ°é“11å·çº¿åœ¨æ—©é«˜å³°å¢åŠ å‡ ç­è½¦ï¼Œç°åœ¨ç­‰ä¸€è¶Ÿçš„æ—¶é—´ä¹Ÿå¤ªé•¿äº†ï¼", 
                height=200,
                placeholder="ä¾‹å¦‚ï¼šä¸Šæµ·åœ°é“çš„ç©ºè°ƒæ˜¯æ‰“ç®—æŠŠäººå†»æ­»å—ï¼Ÿ"
            )
            submitted = st.form_submit_button("ğŸš€ å¼€å§‹åˆ†æ")

    with col2:
        st.header("ğŸ’¡ æ„å›¾ç±»åˆ«è¯´æ˜")
        st.info("""
        - **å’¨è¯¢:** è¯¢é—®ä¿¡æ¯ã€æµç¨‹ç­‰ã€‚
        - **æ±‚åŠ©:** è¯·æ±‚å…·ä½“å¸®åŠ©ï¼ˆå¦‚å¯»ç‰©ï¼‰ã€‚
        - **æŠ•è¯‰:** è¡¨è¾¾å¼ºçƒˆä¸æ»¡ã€‚
        - **ä¸¾æŠ¥:** æ­å‘è¿è§„æˆ–å®‰å…¨éšæ‚£ã€‚
        - **å»ºè®®:** æå‡ºå…·ä½“æ”¹è¿›æ–¹æ¡ˆã€‚
        - **æ­£é¢åé¦ˆ:** æ˜ç¡®çš„èµæ‰¬æˆ–æ„Ÿè°¢ã€‚
        """)

    # --- 5. æ¨¡å‹æ¨ç†ä¸ç»“æœå±•ç¤º (å…¨æ–°å¸ƒå±€) ---
    if submitted:
        if not text_input.strip():
            st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹åå†è¿›è¡Œåˆ†æã€‚")
        else:
            with st.spinner('æ¨¡å‹æ­£åœ¨å…¨åŠ›åˆ†æä¸­ï¼Œè¯·ç¨å€™...'):
                inputs = tokenizer(text_input, return_tensors="pt", truncation=True, padding=True, max_length=256)
                with torch.no_grad():
                    outputs = model(**inputs)
                logits = outputs.logits
                probabilities = F.softmax(logits, dim=-1).flatten().tolist()
                labels = list(model.config.id2label.values())
                prob_df = pd.DataFrame({'æ„å›¾ç±»åˆ«': labels, 'æ¦‚ç‡': probabilities}).sort_values(by='æ¦‚ç‡', ascending=False).reset_index(drop=True)

                st.markdown("---")
                st.header("ğŸ“ˆ åˆ†æç»“æœ")
                
                res_col1, res_col2 = st.columns(2)
                
                with res_col1:
                    # å·¦ä¾§åªä¿ç•™æœ€ç›´è§‚çš„æ¡å½¢å›¾
                    fig = px.bar(
                        prob_df, x='æ¦‚ç‡', y='æ„å›¾ç±»åˆ«', orientation='h', 
                        title='å„æ„å›¾ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒå›¾', text_auto='.2%',
                        color_discrete_sequence=px.colors.sequential.Blues_r
                    )
                    fig.update_layout(
                        yaxis={'categoryorder':'total ascending'},
                        xaxis_title="æ¦‚ç‡",
                        yaxis_title="æ„å›¾ç±»åˆ«",
                        height=400
                    )
                    st.plotly_chart(fig, use_container_width=True)

                with res_col2:
                    # å³ä¾§æä¾›æ·±åº¦è§£è¯»
                    top_prediction = prob_df.iloc[0]
                    st.metric(
                        label="**æœ€é«˜æ¦‚ç‡æ„å›¾**", 
                        value=top_prediction['æ„å›¾ç±»åˆ«'],
                        help=f"æ¨¡å‹å¯¹è¿™ä¸ªåˆ¤æ–­çš„ç½®ä¿¡åº¦ä¸º {top_prediction['æ¦‚ç‡']:.2%}"
                    )
                    
                    st.write("**ç»“æœè§£è¯»:**")
                    interpretation_text = INTERPRETATION_GUIDE.get(top_prediction['æ„å›¾ç±»åˆ«'], "è¿™æ˜¯ä¸€ä¸ªé€šç”¨åé¦ˆã€‚")
                    st.markdown(f"> {interpretation_text}")

                    with st.expander("æŸ¥çœ‹è¯¦ç»†æ¦‚ç‡æ•°æ®"):
                        st.dataframe(prob_df, use_container_width=True, hide_index=True)
                
# --- 6. ä¾§è¾¹æ  (Sidebar) ---
st.sidebar.header("å…³äºæœ¬é¡¹ç›®")
st.sidebar.info(
    """
    è¿™æ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„NLPé¡¹ç›®ï¼Œæ—¨åœ¨é€šè¿‡AIæŠ€æœ¯èµ‹èƒ½åŸå¸‚äº¤é€šç®¡ç†ï¼Œè‡ªåŠ¨åˆ†ææµ·é‡ç¤¾æƒ…æ°‘æ„ï¼Œæå‡å“åº”æ•ˆç‡ã€‚
    
    **æ ¸å¿ƒæŠ€æœ¯æ ˆ:**
    - **æ™ºèƒ½æ ‡æ³¨:** LLM API (`é€šä¹‰åƒé—®`), Prompt Engineering
    - **æ¨¡å‹è®­ç»ƒ:** `PyTorch`, `Hugging Face Transformers`
    - **æ¨¡å‹é€‰å‹:** `MacBERT` vs `RoBERTa` (A/B Test)
    - **æ€§èƒ½ä¼˜åŒ–:** ç±»åˆ«åŠ æƒ, è¶…å‚æ•°æœç´¢ (`Optuna`)
    - **åº”ç”¨æ„å»º:** `Streamlit`, `Plotly`
    """
)
st.sidebar.markdown("---")
st.sidebar.write("**å¼€å‘è€…:** æ½˜ç¦¹èŒ ç§¦èµ«")
st.sidebar.write("**GitHub:** [https://github.com/mengzi667/NLP_TPnews](https://github.com/mengzi667/NLP_TPnews)")
