import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F
import pandas as pd
import plotly.express as px

# --- 1. é¡µé¢é…ç½® ---
st.set_page_config(
    page_title="åŸå¸‚äº¤é€šèˆ†æƒ…æ„å›¾è¯†åˆ«ç³»ç»Ÿ",
    page_icon="ğŸš‡",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- 2. è‡ªå®šä¹‰CSSæ ·å¼ ---
st.markdown("""
<style>
    /* ... (ä½ æ‰€æœ‰çš„CSSæ ·å¼ä¿æŒä¸å˜) ... */
    .main-title { font-size: 2.5rem; ... }
    .subtitle { font-size: 1.1rem; ... }
    /* ... etc ... */
</style>
""", unsafe_allow_html=True)

# --- 3. åŠ è½½æ¨¡å‹ ---
@st.cache_resource
def load_model():
    """ä»Hugging Face HubåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚"""
    model_id = "Mengzi667/macbert-traffic-intent-classifier" 
    try:
        with st.spinner("ğŸ¤– æ­£åœ¨ä»äº‘ç«¯åŠ è½½AIæ¨¡å‹..."):
            tokenizer = AutoTokenizer.from_pretrained(model_id)
            model = AutoModelForSequenceClassification.from_pretrained(model_id)
        st.success("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼", icon="ğŸ‰")
        return tokenizer, model
    except Exception as e:
        st.error("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥", icon="âš ï¸")
        st.error(f"é”™è¯¯è¯¦æƒ…: {e}")
        return None, None

tokenizer, model = load_model()

# --- 4. é¡µé¢æ ‡é¢˜å’Œç®€ä»‹ ---
st.markdown('<h1 class="main-title">ğŸš‡ åŸå¸‚äº¤é€šèˆ†æƒ…æ„å›¾è¯†åˆ«ç³»ç»Ÿ</h1>', unsafe_allow_html=True)
st.markdown('<p class="subtitle">åŸºäºMacBERTæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ™ºèƒ½è¯†åˆ«åŸå¸‚äº¤é€šç›¸å…³æ–‡æœ¬çš„æ ¸å¿ƒæ„å›¾<br>è®©åŸå¸‚äº¤é€šç®¡ç†æ›´æ™ºèƒ½ã€æ›´é«˜æ•ˆ</p>', unsafe_allow_html=True)

# --- 5. ä¸»åº”ç”¨ç•Œé¢ ---
if model is None:
    st.error("ğŸš« æ¨¡å‹æœªèƒ½æˆåŠŸåŠ è½½ï¼Œåº”ç”¨æ— æ³•æ­£å¸¸å·¥ä½œã€‚è¯·åˆ·æ–°é¡µé¢é‡è¯•ã€‚", icon="âš ï¸")
else:
    INTERPRETATION_GUIDE = {
        "æŠ•è¯‰": "æ¨¡å‹è¯†åˆ«å‡ºæ–‡æœ¬ä¸­å«æœ‰å¼ºçƒˆçš„è´Ÿé¢æƒ…ç»ªå’Œå¯¹ç°çŠ¶çš„ä¸æ»¡ï¼Œè¿™é€šå¸¸æŒ‡å‘ä¸€ä¸ªéœ€è¦è¢«è§£å†³çš„é—®é¢˜ã€‚",
        "å»ºè®®": "æ¨¡å‹æ•æ‰åˆ°äº†ä¸€ä¸ªå…·ä½“çš„æ”¹è¿›æƒ³æ³•æˆ–æ–¹æ¡ˆã€‚è¿™ç±»åé¦ˆå¯¹ä¼˜åŒ–æœåŠ¡éå¸¸æœ‰ä»·å€¼ã€‚",
        "å’¨è¯¢": "æ–‡æœ¬çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªä¿¡æ¯é—®è¯¢ï¼Œç”¨æˆ·å¸Œæœ›è·å¾—ä¸€ä¸ªå®¢è§‚çš„ç­”æ¡ˆã€‚",
        "æ±‚åŠ©": "è¿™æ˜¯ä¸€ä¸ªæ˜ç¡®çš„æ±‚åŠ©ä¿¡å·ï¼Œé€šå¸¸ä¸ä¸ªäººé‡åˆ°çš„å…·ä½“å›°éš¾ï¼ˆå¦‚å¤±ç‰©ï¼‰ç›¸å…³ã€‚",
        "ä¸¾æŠ¥": "æ¨¡å‹è¯†åˆ«å‡ºäº†å¯¹æŸä¸ªå…·ä½“è¿è§„è¡Œä¸ºæˆ–å®‰å…¨éšæ‚£çš„æ­å‘ï¼Œéœ€è¦ç›¸å…³éƒ¨é—¨å…³æ³¨ã€‚",
        "æ­£é¢åé¦ˆ": "æ–‡æœ¬è¡¨è¾¾äº†æ˜ç¡®çš„èµæ‰¬æˆ–æ„Ÿè°¢ï¼Œæ˜¯æå‡æœåŠ¡ä¿¡å¿ƒçš„é‡è¦æ¥æºã€‚",
    }

    st.markdown("### ğŸ“ æ–‡æœ¬åˆ†æ")
    
    with st.form("intent_form", clear_on_submit=False):
        text_input = st.text_area(
            label="æ–‡æœ¬å†…å®¹",
            value="å¼ºçƒˆå»ºè®®åœ°é“11å·çº¿åœ¨æ—©é«˜å³°å¢åŠ å‡ ç­è½¦ï¼Œç°åœ¨ç­‰ä¸€è¶Ÿçš„æ—¶é—´ä¹Ÿå¤ªé•¿äº†ï¼",
            height=120,
            placeholder="ä¾‹å¦‚ï¼šä¸Šæµ·åœ°é“çš„ç©ºè°ƒæ˜¯æ‰“ç®—æŠŠäººå†»æ­»å—ï¼Ÿ\næˆ–è€…ï¼šè¯·é—®åœ°é“æœ«ç­è½¦æ˜¯å‡ ç‚¹ï¼Ÿ",
            help="ğŸ’¬ å’¨è¯¢ | ğŸ†˜ æ±‚åŠ© | ğŸ˜¤ æŠ•è¯‰ | ğŸš¨ ä¸¾æŠ¥ | ğŸ’¡ å»ºè®® | ğŸ‘ æ­£é¢åé¦ˆ",
            label_visibility="collapsed"
        )
        submitted = st.form_submit_button("ğŸš€ å¼€å§‹æ™ºèƒ½åˆ†æ", use_container_width=True)

    # --- 6. æ¨¡å‹æ¨ç†ä¸ç»“æœå±•ç¤º ---
    if submitted:
        if not text_input.strip():
            st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹åå†è¿›è¡Œåˆ†æã€‚", icon="ğŸ“")
        else:
            with st.spinner('ğŸ¤– AIæ­£åœ¨æ·±åº¦åˆ†æä¸­ï¼Œè¯·ç¨å€™...'):
                inputs = tokenizer(text_input, return_tensors="pt", truncation=True, padding=True, max_length=256)
                with torch.no_grad():
                    outputs = model(**inputs)
                logits = outputs.logits
                probabilities = F.softmax(logits, dim=-1).flatten().tolist()
                labels = list(model.config.id2label.values())
                prob_df = pd.DataFrame({'æ„å›¾ç±»åˆ«': labels, 'æ¦‚ç‡': probabilities}).sort_values(by='æ¦‚ç‡', ascending=False).reset_index(drop=True)

                st.markdown("---")
                st.markdown("### ğŸ“Š åˆ†æç»“æœ")
                
                result_col1, result_col2 = st.columns([1.5, 1], gap="large")
                
                with result_col1:
                    fig = px.bar(
                        prob_df, 
                        x='æ¦‚ç‡', y='æ„å›¾ç±»åˆ«', orientation='h',
                        title='å„æ„å›¾ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒ', text=[f'{p:.1%}' for p in prob_df['æ¦‚ç‡']],
                        color='æ¦‚ç‡', color_continuous_scale='Blues'
                    )
                    fig.update_layout(
                        yaxis={'categoryorder':'total ascending', 'showgrid': False, 'title': ""},
                        height=350, showlegend=False, title_x=0.5,
                        plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)',
                        font=dict(size=12), margin=dict(l=20, r=20, t=40, b=20),
                        xaxis=dict(showgrid=True, gridcolor='rgba(0,0,0,0.1)', title="")
                    )
                    fig.update_traces(textposition='outside')
                    
                    # --- !!! æ ¸å¿ƒä¿®æ­£ï¼šæ·»åŠ ä¸‹é¢è¿™è¡Œä»£ç  !!! ---
                    st.plotly_chart(fig, use_container_width=True)
                    # ---------------------------------------------

                with result_col2:
                    top_prediction = prob_df.iloc[0]
                    confidence = top_prediction['æ¦‚ç‡']
                    
                    st.markdown(f"""
                    <div class="result-card">
                        <h3 style="margin: 0 0 0.5rem 0;">ğŸ¯ è¯†åˆ«ç»“æœ</h3>
                        <h2 style="margin: 0 0 0.5rem 0;">{top_prediction['æ„å›¾ç±»åˆ«']}</h2>
                        <p style="margin: 0; font-size: 1.1rem;">ç½®ä¿¡åº¦: {confidence:.1%}</p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    interpretation_text = INTERPRETATION_GUIDE.get(top_prediction['æ„å›¾ç±»åˆ«'], "è¿™æ˜¯ä¸€ä¸ªé€šç”¨åé¦ˆã€‚")
                    st.markdown(f'<div class="interpretation-text">ğŸ’¡ <strong>æ™ºèƒ½è§£è¯»:</strong><br>{interpretation_text}</div>', unsafe_allow_html=True)

                    with st.expander("ğŸ“Š è¯¦ç»†æ•°æ®", expanded=False):
                        for idx, row in prob_df.iterrows():
                            percentage = row['æ¦‚ç‡']
                            st.markdown(f"""
                            <div style="display: flex; justify-content: space-between; align-items: center; padding: 0.5rem; margin-bottom: 0.25rem; background: #f8fafc; border-radius: 6px;">
                                <span>{row['æ„å›¾ç±»åˆ«']}</span>
                                <span style="font-weight: bold; color: #667eea;">{percentage:.1%}</span>
                            </div>
                            """, unsafe_allow_html=True)

# --- 7. ç®€åŒ–çš„ä¾§è¾¹æ  ---
with st.sidebar:
    st.markdown("### ğŸ”¬ é¡¹ç›®ä¿¡æ¯")
    
    st.markdown("""
    <div style="background: white; padding: 1rem; border-radius: 8px; margin-bottom: 1rem;">
        <p style="margin-bottom: 0; color: #6b7280; font-size: 0.9rem;">
            åŸºäºMacBERTæ·±åº¦å­¦ä¹ æ¨¡å‹çš„åŸå¸‚äº¤é€šèˆ†æƒ…æ„å›¾è¯†åˆ«ç³»ç»Ÿï¼Œ
            è‡ªåŠ¨åˆ†æç¤¾æƒ…æ°‘æ„ï¼Œæå‡ç®¡ç†æ•ˆç‡ã€‚
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("**ğŸ› ï¸ æ ¸å¿ƒæŠ€æœ¯**")
    st.markdown("""
    - **æ¨¡å‹:** MacBERT + PyTorch
    - **ä¼˜åŒ–:** ç±»åˆ«åŠ æƒ + Optuna
    - **éƒ¨ç½²:** Streamlit + Plotly
    """)
    
    st.markdown("---")
    
    st.markdown("**ğŸ‘¨â€ğŸ’» å¼€å‘å›¢é˜Ÿ**")
    st.markdown("æ½˜ç¦¹èŒ & ç§¦èµ«")
    
    st.markdown("**ğŸ”— é¡¹ç›®é“¾æ¥**")
    st.markdown("[GitHub ä»“åº“](https://github.com/mengzi667/NLP_TPnews)")
    
    st.markdown("---")
    st.info("ğŸ’¡ æ”¯æŒè¯†åˆ«ï¼šå’¨è¯¢ã€æ±‚åŠ©ã€æŠ•è¯‰ã€ä¸¾æŠ¥ã€å»ºè®®ã€æ­£é¢åé¦ˆ")